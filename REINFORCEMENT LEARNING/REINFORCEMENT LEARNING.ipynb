{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725c735c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "     -------------------------------------- 721.7/721.7 kB 1.9 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\hi\\anaconda3\\lib\\site-packages (from gym) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\hi\\anaconda3\\lib\\site-packages (from gym) (2.0.0)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml): started\n",
      "  Building wheel for gym (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827633 sha256=9b931c4130b0cbb395a424325ee4b688f50494b526bcbc1f870815481c9cc426\n",
      "  Stored in directory: c:\\users\\hi\\appdata\\local\\pip\\cache\\wheels\\ae\\5f\\67\\64914473eb34e9ba89dbc7eefe7e9be8f6673fbc6f0273b29f\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, gym\n",
      "Successfully installed gym-0.26.2 gym-notices-0.0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6cfde98",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Choose an action greedily from the Q-table with added noise\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mQ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (episode \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Take the action and observe the outcome\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Define the environment\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "\n",
    "# Initialize Q-table with zeros\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.8\n",
    "discount_factor = 0.95\n",
    "num_episodes = 2000\n",
    "\n",
    "# List to store rewards\n",
    "rewards = []\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Choose an action greedily from the Q-table with added noise\n",
    "        action = np.argmax(Q[state, :] + np.random.randn(1, env.action_space.n) * (1 / (episode + 1)))\n",
    "        \n",
    "        # Take the action and observe the outcome\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Update Q-value using the Q-learning formula\n",
    "        Q[state, action] = Q[state, action] + learning_rate * (reward + discount_factor * np.max(Q[next_state, :]) - Q[state, action])\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Evaluate the trained agent\n",
    "total_rewards = 0\n",
    "num_episodes = 100\n",
    "for _ in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Choose action greedily from Q-table\n",
    "        action = np.argmax(Q[state, :])\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_rewards += reward\n",
    "\n",
    "average_reward = total_rewards / num_episodes\n",
    "print(f\"Average reward over {num_episodes} episodes: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1233548b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Choose an action greedily from the Q-table with added noise\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(Q[\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m, :] \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (episode \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Take the action and observe the outcome\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'tuple'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Define the environment\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "\n",
    "# Initialize Q-table with zeros\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.8\n",
    "discount_factor = 0.95\n",
    "num_episodes = 2000\n",
    "\n",
    "# List to store rewards\n",
    "rewards = []\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Choose an action greedily from the Q-table with added noise\n",
    "        action = np.argmax(Q[int(state), :] + np.random.randn(1, env.action_space.n) * (1 / (episode + 1)))\n",
    "        \n",
    "        # Take the action and observe the outcome\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Update Q-value using the Q-learning formula\n",
    "        Q[int(state), action] = Q[int(state), action] + learning_rate * (reward + discount_factor * np.max(Q[int(next_state), :]) - Q[int(state), action])\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Evaluate the trained agent\n",
    "total_rewards = 0\n",
    "num_episodes = 100\n",
    "for _ in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "    # Choose an action greedily from the Q-table with added noise\n",
    "        action = np.argmax(Q[int(state[0]), :] + np.random.randn(1, env.action_space.n) * (1 / (episode + 1)))\n",
    "    \n",
    "    # Take the action and observe the outcome\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    # Update Q-value using the Q-learning formula\n",
    "        Q[int(state[0]), action] = Q[int(state[0]), action] + learning_rate * (reward + discount_factor * np.max(Q[int(next_state), :]) - Q[int(state[0]), action])\n",
    "    \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "\n",
    "average_reward = total_rewards / num_episodes\n",
    "print(f\"Average reward over {num_episodes} episodes: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dfd1d0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m     20\u001b[0m     state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m---> 21\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Convert state to integer\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     23\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'tuple'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Define the environment\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "\n",
    "# Initialize Q-table with zeros\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.8\n",
    "discount_factor = 0.95\n",
    "num_episodes = 2000\n",
    "\n",
    "# List to store rewards\n",
    "rewards = []\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = int(state)  # Convert state to integer\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Choose an action greedily from the Q-table with added noise\n",
    "        action = np.argmax(Q[state, :] + np.random.randn(1, env.action_space.n) * (1 / (episode + 1)))\n",
    "        \n",
    "        # Take the action and observe the outcome\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = int(next_state)  # Convert next_state to integer\n",
    "        \n",
    "        # Update Q-value using the Q-learning formula\n",
    "        Q[state, action] = Q[state, action] + learning_rate * (reward + discount_factor * np.max(Q[next_state, :]) - Q[state, action])\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Evaluate the trained agent\n",
    "total_rewards = 0\n",
    "num_episodes = 100\n",
    "for _ in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = int(state)  # Convert state to integer\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Choose action greedily from Q-table\n",
    "        action = np.argmax(Q[state, :])\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        state = int(state)  # Convert state to integer\n",
    "        total_rewards += reward\n",
    "\n",
    "average_reward = total_rewards / num_episodes\n",
    "print(f\"Average reward over {num_episodes} episodes: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5271d776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward after 1000 steps: 870.8036425380701\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultiArmedBandit:\n",
    "    def __init__(self, num_arms):\n",
    "        self.num_arms = num_arms\n",
    "        self.true_rewards = np.random.uniform(0, 1, num_arms)\n",
    "        self.estimated_rewards = np.zeros(num_arms)\n",
    "        self.action_counts = np.zeros(num_arms)\n",
    "\n",
    "    def select_action(self, epsilon):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            # Explore: choose a random arm\n",
    "            action = np.random.randint(self.num_arms)\n",
    "        else:\n",
    "            # Exploit: choose the arm with the highest estimated reward\n",
    "            action = np.argmax(self.estimated_rewards)\n",
    "        return action\n",
    "\n",
    "    def update_estimates(self, action, reward):\n",
    "        self.action_counts[action] += 1\n",
    "        self.estimated_rewards[action] += (reward - self.estimated_rewards[action]) / self.action_counts[action]\n",
    "\n",
    "# Main function\n",
    "def main(num_arms, num_steps, epsilon):\n",
    "    bandit = MultiArmedBandit(num_arms)\n",
    "    total_reward = 0\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        action = bandit.select_action(epsilon)\n",
    "        reward = np.random.normal(bandit.true_rewards[action], 1)  # Sample reward from a normal distribution\n",
    "        total_reward += reward\n",
    "        bandit.update_estimates(action, reward)\n",
    "\n",
    "    print(f\"Total reward after {num_steps} steps: {total_reward}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    num_arms = 5\n",
    "    num_steps = 1000\n",
    "    epsilon = 0.1\n",
    "    main(num_arms, num_steps, epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9e677f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1:\n",
      "Exploiting: Chose arm 0\n",
      "Received reward: -0.035477936343178196, Updated estimated rewards: [-0.03547794  0.          0.          0.          0.        ]\n",
      "\n",
      "Step 2:\n",
      "Exploiting: Chose arm 1\n",
      "Received reward: -1.0849584233893448, Updated estimated rewards: [-0.03547794 -1.08495842  0.          0.          0.        ]\n",
      "\n",
      "Step 3:\n",
      "Exploiting: Chose arm 2\n",
      "Received reward: -1.1205182203612765, Updated estimated rewards: [-0.03547794 -1.08495842 -1.12051822  0.          0.        ]\n",
      "\n",
      "Step 4:\n",
      "Exploiting: Chose arm 3\n",
      "Received reward: -0.2818729308835977, Updated estimated rewards: [-0.03547794 -1.08495842 -1.12051822 -0.28187293  0.        ]\n",
      "\n",
      "Step 5:\n",
      "Exploiting: Chose arm 4\n",
      "Received reward: 1.4541378120985562, Updated estimated rewards: [-0.03547794 -1.08495842 -1.12051822 -0.28187293  1.45413781]\n",
      "\n",
      "Step 6:\n",
      "Exploring: Chose arm 4\n",
      "Received reward: -0.05776121841084936, Updated estimated rewards: [-0.03547794 -1.08495842 -1.12051822 -0.28187293  0.6981883 ]\n",
      "\n",
      "Step 7:\n",
      "Exploiting: Chose arm 4\n",
      "Received reward: -0.9934806710451557, Updated estimated rewards: [-0.03547794 -1.08495842 -1.12051822 -0.28187293  0.13429864]\n",
      "\n",
      "Step 8:\n",
      "Exploiting: Chose arm 4\n",
      "Received reward: 0.23880367450333495, Updated estimated rewards: [-0.03547794 -1.08495842 -1.12051822 -0.28187293  0.1604249 ]\n",
      "\n",
      "Step 9:\n",
      "Exploiting: Chose arm 4\n",
      "Received reward: 0.465347528470478, Updated estimated rewards: [-0.03547794 -1.08495842 -1.12051822 -0.28187293  0.22140943]\n",
      "\n",
      "Step 10:\n",
      "Exploiting: Chose arm 4\n",
      "Received reward: 0.6093873774508998, Updated estimated rewards: [-0.03547794 -1.08495842 -1.12051822 -0.28187293  0.28607242]\n",
      "\n",
      "Total reward after 10 steps: -0.8063930079101334\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultiArmedBandit:\n",
    "    def __init__(self, num_arms):\n",
    "        self.num_arms = num_arms\n",
    "        self.true_rewards = np.random.uniform(0, 1, num_arms)\n",
    "        self.estimated_rewards = np.zeros(num_arms)\n",
    "        self.action_counts = np.zeros(num_arms)\n",
    "\n",
    "    def select_action(self, epsilon):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            # Explore: choose a random arm\n",
    "            action = np.random.randint(self.num_arms)\n",
    "            print(f\"Exploring: Chose arm {action}\")\n",
    "        else:\n",
    "            # Exploit: choose the arm with the highest estimated reward\n",
    "            action = np.argmax(self.estimated_rewards)\n",
    "            print(f\"Exploiting: Chose arm {action}\")\n",
    "        return action\n",
    "\n",
    "    def update_estimates(self, action, reward):\n",
    "        self.action_counts[action] += 1\n",
    "        self.estimated_rewards[action] += (reward - self.estimated_rewards[action]) / self.action_counts[action]\n",
    "        print(f\"Received reward: {reward}, Updated estimated rewards: {self.estimated_rewards}\")\n",
    "\n",
    "# Main function\n",
    "def main(num_arms, num_steps, epsilon):\n",
    "    bandit = MultiArmedBandit(num_arms)\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(1, num_steps + 1):\n",
    "        print(f\"\\nStep {step}:\")\n",
    "        action = bandit.select_action(epsilon)\n",
    "        reward = np.random.normal(bandit.true_rewards[action], 1)  # Sample reward from a normal distribution\n",
    "        total_reward += reward\n",
    "        bandit.update_estimates(action, reward)\n",
    "\n",
    "    print(f\"\\nTotal reward after {num_steps} steps: {total_reward}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    num_arms = 5\n",
    "    num_steps = 10\n",
    "    epsilon = 0.1\n",
    "    main(num_arms, num_steps, epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18dc8b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -340\n",
      "Episode 50, Total Reward: 0\n",
      "Episode 100, Total Reward: 0\n",
      "Episode 150, Total Reward: 0\n",
      "Episode 200, Total Reward: 0\n",
      "Episode 250, Total Reward: 0\n",
      "Episode 300, Total Reward: 0\n",
      "Episode 350, Total Reward: 0\n",
      "Episode 400, Total Reward: 0\n",
      "Episode 450, Total Reward: 0\n",
      "\n",
      "Learned Q-values:\n",
      "State (0,0): [-1.81271664 -1.8568406  -1.84268311 -1.8189388 ]\n",
      "State (0,1): [-1.7222929  -1.75754453 -1.6889057  -1.76432583]\n",
      "State (0,2): [-1.57056807 -1.60417161 -1.58321692 -1.62075518]\n",
      "State (0,3): [-1.39941645 -1.44224286 -1.3946537  -1.47080498]\n",
      "State (0,4): [-1.31254187 -1.31276756 -1.29152553 -1.27140949]\n",
      "State (0,5): [-1.13499576 -1.15913825 -1.10089278 -1.17794458]\n",
      "State (0,6): [-0.95617925 -1.01087514 -1.01849441 -0.96680277]\n",
      "State (0,7): [-0.86482753 -0.86793874 -0.84463278 -0.95167764]\n",
      "State (0,8): [-0.76487095 -0.80914903 -0.7379466  -0.74065781]\n",
      "State (0,9): [-0.55996103 -0.56868716 -0.64197399 -0.6321551 ]\n",
      "State (0,10): [-0.3940399  -0.4897576  -0.48071071 -0.40905016]\n",
      "State (0,11): [-0.3940399  -0.3940399  -0.47926    -0.39404646]\n",
      "State (1,0): [-1.90158188 -1.92286279 -1.97249195 -1.89782508]\n",
      "State (1,1): [-1.74474823 -1.7399493  -1.88331252 -1.81218776]\n",
      "State (1,2): [-1.56872195 -1.59432452 -1.56750784 -1.65727726]\n",
      "State (1,3): [-1.40450748 -1.43766236 -1.44518635 -1.38504262]\n",
      "State (1,4): [-1.23450657 -1.2995841  -1.25675903 -1.27723933]\n",
      "State (1,5): [-1.13748491 -1.09646748 -1.06309118 -1.07532501]\n",
      "State (1,6): [-0.87815973 -0.95467808 -0.98506994 -0.95740754]\n",
      "State (1,7): [-0.87771378 -0.79847734 -0.82777536 -0.8335705 ]\n",
      "State (1,8): [-0.77528203 -0.72634401 -0.63240883 -0.72461233]\n",
      "State (1,9): [-0.57101494 -0.55452065 -0.63492513 -0.6450615 ]\n",
      "State (1,10): [-0.39404646 -0.3933109  -0.44527774 -0.49877895]\n",
      "State (1,11): [-0.3940399 -0.38368   -0.3439    -0.40636  ]\n",
      "State (2,0): [-2.11539803 -2.14512448 -2.17154386 -2.14266739]\n",
      "State (2,1): [ -1.89198864  -1.90783247 -19.20242366  -1.89423763]\n",
      "State (2,2): [ -1.64361881  -1.60005851 -19.24885787  -1.66203337]\n",
      "State (2,3): [ -1.42597293  -1.34940764 -27.444452    -1.43493606]\n",
      "State (2,4): [ -1.26006269  -1.21209525 -19.2301425   -1.19064315]\n",
      "State (2,5): [ -1.00293051  -1.01603085 -27.40967779  -1.10594804]\n",
      "State (2,6): [ -0.80793732  -0.87123382 -34.99816814  -0.85869619]\n",
      "State (2,7): [ -0.80240971  -0.78303056 -10.13108279  -0.75640969]\n",
      "State (2,8): [ -0.69785179  -0.61112954 -10.1079124   -0.65232189]\n",
      "State (2,9): [ -0.5079994   -0.52291099 -10.15644824  -0.55081819]\n",
      "State (2,10): [ -0.30349     -0.3439     -10.          -0.34047503]\n",
      "State (2,11): [-0.29701 -0.1      0.       0.     ]\n",
      "State (3,0): [ -2.0307457  -43.02913405  -0.06679858 -28.79612191]\n",
      "State (3,1): [0. 0. 0. 0.]\n",
      "State (3,2): [0. 0. 0. 0.]\n",
      "State (3,3): [0. 0. 0. 0.]\n",
      "State (3,4): [0. 0. 0. 0.]\n",
      "State (3,5): [0. 0. 0. 0.]\n",
      "State (3,6): [0. 0. 0. 0.]\n",
      "State (3,7): [0. 0. 0. 0.]\n",
      "State (3,8): [0. 0. 0. 0.]\n",
      "State (3,9): [0. 0. 0. 0.]\n",
      "State (3,10): [0. 0. 0. 0.]\n",
      "State (3,11): [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CliffWalking:\n",
    "    def __init__(self, rows, cols):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.start = (rows-1, 0)\n",
    "        self.goal = (rows-1, cols-1)\n",
    "        self.cliff = [(rows-1, j) for j in range(1, cols-1)]\n",
    "        self.actions = ['UP', 'RIGHT', 'DOWN', 'LEFT']\n",
    "        self.num_actions = len(self.actions)\n",
    "        self.state = self.start\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Takes action as input and returns next_state, reward, and whether it's terminal state or not.\n",
    "        \"\"\"\n",
    "        if action == 'UP':\n",
    "            next_state = (self.state[0] - 1, self.state[1])\n",
    "        elif action == 'RIGHT':\n",
    "            next_state = (self.state[0], self.state[1] + 1)\n",
    "        elif action == 'DOWN':\n",
    "            next_state = (self.state[0] + 1, self.state[1])\n",
    "        elif action == 'LEFT':\n",
    "            next_state = (self.state[0], self.state[1] - 1)\n",
    "\n",
    "        if next_state[0] < 0 or next_state[0] >= self.rows or next_state[1] < 0 or next_state[1] >= self.cols:\n",
    "            next_state = self.state\n",
    "\n",
    "        if next_state in self.cliff:\n",
    "            reward = -100\n",
    "            next_state = self.start\n",
    "            is_done = False\n",
    "        elif next_state == self.goal:\n",
    "            reward = 0\n",
    "            is_done = True\n",
    "        else:\n",
    "            reward = -1\n",
    "            is_done = False\n",
    "\n",
    "        self.state = next_state\n",
    "        return next_state, reward, is_done\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    rows, cols = 4, 12\n",
    "    env = CliffWalking(rows, cols)\n",
    "    Q = np.zeros((rows, cols, env.num_actions))\n",
    "    num_episodes = 500\n",
    "    epsilon = 0.1\n",
    "    learning_rate = 0.1\n",
    "    discount_factor = 0.9\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.start\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.choice(env.actions)\n",
    "            else:\n",
    "                action = env.actions[np.argmax(Q[state[0], state[1], :])]\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_action = np.argmax(Q[next_state[0], next_state[1], :])\n",
    "            Q[state[0], state[1], env.actions.index(action)] += learning_rate * (\n",
    "                        reward + discount_factor * Q[next_state[0], next_state[1], next_action] -\n",
    "                        Q[state[0], state[1], env.actions.index(action)])\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        if episode % 50 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "    # Display learned Q-values for a few states\n",
    "    print(\"\\nLearned Q-values:\")\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            print(f\"State ({i},{j}): {Q[i, j, :]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b126337",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "negative dimensions are not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m env_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     57\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m---> 58\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 36\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(env_name, num_episodes)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(env_name, num_episodes):\n\u001b[0;32m     35\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(env_name)\n\u001b[1;32m---> 36\u001b[0m     agent \u001b[38;5;241m=\u001b[39m \u001b[43mQLearningAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m     39\u001b[0m         state \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mdiscretize_state(env\u001b[38;5;241m.\u001b[39mreset())\n",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m, in \u001b[0;36mQLearningAgent.__init__\u001b[1;34m(self, env, learning_rate, discount_factor, epsilon)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_space_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m((env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mhigh \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mones(env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape))\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m))\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Initialize Q-table\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_space_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: negative dimensions are not allowed"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, learning_rate=0.1, discount_factor=0.9, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.action_space_size = env.action_space.n\n",
    "        self.state_space_size = tuple((env.observation_space.high + np.ones(env.observation_space.shape)).astype(int))\n",
    "\n",
    "        # Initialize Q-table\n",
    "        self.q_table = np.zeros((self.state_space_size + (self.action_space_size,)))\n",
    "\n",
    "    def discretize_state(self, state):\n",
    "        state_bins = [np.linspace(env.observation_space.low[i], env.observation_space.high[i], num=self.state_space_size[i]) for i in range(env.observation_space.shape[0])]\n",
    "        state_discretized = [np.digitize(state[i], state_bins[i]) - 1 for i in range(env.observation_space.shape[0])]\n",
    "        return tuple(state_discretized)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return self.env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  # Exploit\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.learning_rate * td_error\n",
    "\n",
    "# Main function\n",
    "def main(env_name, num_episodes):\n",
    "    env = gym.make(env_name)\n",
    "    agent = QLearningAgent(env)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = agent.discretize_state(env.reset())\n",
    "\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = agent.discretize_state(next_state)\n",
    "            agent.update_q_table(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    env_name = \"CartPole-v1\"\n",
    "    num_episodes = 100\n",
    "    main(env_name, num_episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53dbbfa1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m env_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m---> 61\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 42\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(env_name, num_episodes)\u001b[0m\n\u001b[0;32m     39\u001b[0m agent \u001b[38;5;241m=\u001b[39m QLearningAgent(env)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[1;32m---> 42\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscretize_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     45\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m, in \u001b[0;36mQLearningAgent.discretize_state\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdiscretize_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m---> 20\u001b[0m     state_bins \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mlinspace(env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mlow[i], env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mhigh[i], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_bins[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)]\n\u001b[0;32m     21\u001b[0m     state_discretized \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mdigitize(state[i], state_bins[i]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(state_discretized)\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, learning_rate=0.1, discount_factor=0.9, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.action_space_size = env.action_space.n\n",
    "\n",
    "        # Discretize observation space\n",
    "        self.observation_bins = [20, 20, 20, 20]\n",
    "        self.state_space_size = (self.observation_bins + [self.action_space_size])\n",
    "\n",
    "        # Initialize Q-table\n",
    "        self.q_table = np.zeros(self.state_space_size)\n",
    "\n",
    "    def discretize_state(self, state):\n",
    "        state_bins = [np.linspace(env.observation_space.low[i], env.observation_space.high[i], self.observation_bins[i]) for i in range(env.observation_space.shape[0])]\n",
    "        state_discretized = [np.digitize(state[i], state_bins[i]) - 1 for i in range(env.observation_space.shape[0])]\n",
    "        return tuple(state_discretized)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return self.env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  # Exploit\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.learning_rate * td_error\n",
    "\n",
    "# Main function\n",
    "def main(env_name, num_episodes):\n",
    "    env = gym.make(env_name)\n",
    "    agent = QLearningAgent(env)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = agent.discretize_state(env.reset())\n",
    "\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = agent.discretize_state(next_state)\n",
    "            agent.update_q_table(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    env_name = \"CartPole-v1\"\n",
    "    num_episodes = 100\n",
    "    main(env_name, num_episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237374da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
